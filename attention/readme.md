# Attention

##### TODO

- CUDA core FlashAttention
- cutlas FlashAttention

##### reference

- [怎么加快大模型推理？10分钟学懂VLLM内部原理，KV Cache，PageAttention](https://www.bilibili.com/video/BV1kx4y1x7bu/?spm_id_from=333.337.search-card.all.click&vd_source=d99fb874fa9e85fe5793ec3fa65ab064)
- [E07 | Fast LLM Serving with vLLM and PagedAttention](https://www.youtube.com/watch?v=Oq2SN7uutbQ)

- [FlashAttention 笔记：符号表示](https://zhuanlan.zhihu.com/p/708867810)

- [FlashAttention fp8实现（ada架构)](https://zhuanlan.zhihu.com/p/712314257)

  
